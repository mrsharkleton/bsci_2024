{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSCI AI Training Practical Session\n",
    "**Michael Sharkey**  \n",
    "Wellcome Trust 4WardNorth Doctoral Fellow  \n",
    "Senior Research Scientist  \n",
    "University of Sheffield and Sheffield Teaching Hospitals  \n",
    "m.j.sharkey@sheffield.ac.uk  \n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mrsharkleton/bsci_2024/blob/main/training_demo.ipynb)\n",
    "   \n",
    "## Aims: \n",
    "Train a segmentation algorithm for cardiac structures on CTPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any required packages\n",
    "%pip install gdown nibabel \"numpy<2\" matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch if needed\n",
    "%pip install torch==2.4.1+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MONAI a medical imaging AI framework\n",
    "%pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "import gdown\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImageD, EnsureChannelFirstD, SpacingD, ScaleIntensityRangeD,\n",
    "    RandCropByPosNegLabelD, RandFlipD, RandShiftIntensityD, RandRotate90D, EnsureTypeD, SpatialPadD, AsDiscreteD\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import DataLoader, CacheDataset\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils import first\n",
    "from monai.networks.layers import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset we will be using\n",
    "use_temp_dir = True\n",
    "if use_temp_dir:\n",
    "    work_dir = tempfile.mkdtemp()\n",
    "else:\n",
    "    work_dir = \"/home/mjs/src/bsci_2024/demo\"\n",
    "print(f\"Working directory: {work_dir}\")\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# Download the dataset as a zip file\n",
    "zip_file_path = f'{work_dir}/bsci_data.zip'\n",
    "extract_dir = f'{work_dir}'\n",
    "\n",
    "if not os.path.exists(zip_file_path):\n",
    "    gdown.download(f\"https://drive.google.com/uc?id=1K5eNftRkYxn7LOIfMghFSNSprubbR-o9\", output=zip_file_path)#, quiet=False, use_cookies=False)\n",
    "\n",
    "# Unzip the dataset\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the extracted directory\n",
    "image_dir = f'{extract_dir}/niis'\n",
    "images = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('image.nii.gz')])\n",
    "labels = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('label.nii.gz')])\n",
    "\n",
    "# check that your images and labels are in the same order!!\n",
    "for (image, label) in (zip(images, labels)):\n",
    "    print(f\"Image: {image}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK YOUR DATA!!!!\n",
    "\n",
    "Actually look at your data and check visually if anything has gone amiss between your images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and label\n",
    "img = nib.load(os.path.join(image_dir, images[0]))\n",
    "label = nib.load(os.path.join(image_dir, labels[0]))\n",
    "\n",
    "# Get image and label data\n",
    "img_data = img.get_fdata()\n",
    "label_data = label.get_fdata()\n",
    "\n",
    "print(f\"Image shape: {img_data.shape}\")\n",
    "print(f\"Label shape: {label_data.shape}\")\n",
    "\n",
    "# Display the image and label\n",
    "n = 4  # number of slices\n",
    "fig, ax = plt.subplots(n, 3, figsize=(15, 10))\n",
    "\n",
    "for i in range(n):\n",
    "    # Show image slice\n",
    "    ax[i, 0].imshow(img_data[:, :, i* img_data.shape[2]//n].T, cmap='gray')\n",
    "    ax[i, 0].set_title('Image')\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    # Show label slice\n",
    "    ax[i, 1].imshow(label_data[:, :, i* label_data.shape[2]//n].T, cmap='viridis')\n",
    "    ax[i, 1].set_title('Label')\n",
    "    ax[i, 1].axis('off')\n",
    "\n",
    "    # Overlay label on image\n",
    "    ax[i, 2].imshow(img_data[:, :, i* img_data.shape[2]//n].T, cmap='gray')\n",
    "    ax[i, 2].imshow(label_data[:, :, i* label_data.shape[2]//n].T, cmap='viridis', alpha=0.5)\n",
    "    ax[i, 2].set_title('Overlay')\n",
    "    ax[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into training, validation and testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(images, labels)]\n",
    "train_files, val_files, test_files = data_dicts[:10], data_dicts[10:15], data_dicts[15:]  # Split into training and validation sets\n",
    "\n",
    "# Display the number of training and validation samples\n",
    "print(f\"Training set: {len(train_files)} samples\")\n",
    "print(f\"Validation set: {len(val_files)} samples\")\n",
    "print(f\"Test set: {len(test_files)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data loader\n",
    "This will handles loading the data and doing any data augmentations for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for derministic training\n",
    "random_seed = 42\n",
    "monai.utils.set_determinism(seed=random_seed)\n",
    "\n",
    "# Define transformations for data augmentation and preprocessing\n",
    "train_transforms = Compose([\n",
    "    LoadImageD(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstD(keys=[\"image\", \"label\"]),\n",
    "    SpacingD(keys=[\"image\", \"label\"], pixdim=(5, 5, 5), mode=(\"bilinear\", \"nearest\")),\n",
    "    ScaleIntensityRangeD(keys=[\"image\"],\n",
    "                    a_min=-1000, a_max=1500, b_min=0.0, b_max=1.0, clip=True),\n",
    "    SpatialPadD(keys=[\"image\", \"label\"], spatial_size=(64,64,64)),\n",
    "    RandCropByPosNegLabelD(keys=[\"image\", \"label\"], label_key=\"label\", spatial_size=(64,64,64), pos=1, neg=1, num_samples=2),\n",
    "    RandFlipD(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=[0]),\n",
    "    RandShiftIntensityD(keys=\"image\", offsets=0.1, prob=0.5),\n",
    "    RandRotate90D(keys=[\"image\", \"label\"], prob=0.5, max_k=3),\n",
    "    AsDiscreteD(keys=[\"label\"], to_onehot=10),\n",
    "    EnsureTypeD(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImageD(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstD(keys=[\"image\", \"label\"]),\n",
    "    SpacingD(keys=[\"image\", \"label\"], pixdim=(5, 5, 5), mode=(\"bilinear\", \"nearest\")),\n",
    "    ScaleIntensityRangeD(keys=[\"image\"],\n",
    "                    a_min=-1000, a_max=1500, b_min=0.0, b_max=1.0, clip=True),\n",
    "    AsDiscreteD(keys=[\"label\"], to_onehot=10),\n",
    "    EnsureTypeD(keys=[\"image\", \"label\"])\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0)\n",
    "train_loader = DataLoader(train_ds, batch_size=10, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=1)\n",
    "\n",
    "test_ds = CacheDataset(data=test_files, transform=val_transforms, cache_rate=1.0)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our ML model, loss function and evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3D U-Net model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=10,  # Background plus our 9 classes\n",
    "    channels=(16, 32, 64, 128),  # Feature map channels\n",
    "    strides=(2, 2, 2),  # Downsample with strides\n",
    "    num_res_units=2,  # Residual units\n",
    "    norm=Norm.BATCH,  # Use batch normalization\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_function = DiceCELoss(softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define evaluation metrics\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "max_epochs = 100 # Train for 100 epochs\n",
    "val_interval = 5  # Evaluate the model every 5 epochs\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "metric_values = []\n",
    "epoch_loss_values = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_loader)} loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_dice = 0\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                # here we need to do a sliding window inference on the entire image\n",
    "                val_outputs = sliding_window_inference(val_inputs, (64, 64, 64), 4, model, overlap=0.25)\n",
    "                val_outputs = torch.softmax(val_outputs, 1)  # Apply softmax\n",
    "                val_outputs = torch.argmax(val_outputs, dim=1, keepdim=True)  # Get class predictions\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "            metric_values.append(metric)\n",
    "            print(f\"epoch {epoch + 1} validation mean dice: {metric:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"best_metric_model.pth\")\n",
    "                print(\"Saved new best metric model\")\n",
    "\n",
    "                # Let's save the first validation image, label, and model output\n",
    "                val_data = first(val_loader)\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                val_outputs = sliding_window_inference(val_inputs, (64, 64, 64), 4, model, overlap=0.25)\n",
    "                val_outputs = torch.softmax(val_outputs, 1)  # Apply softmax\n",
    "                val_outputs = torch.argmax(val_outputs, dim=1, keepdim=True)  # Get class predictions\n",
    "                # Convert output and label to numpy arrays and undo one-hot encoding\n",
    "                val_outputs = val_outputs.detach().cpu().numpy().squeeze()\n",
    "                val_labels = val_labels.detach().cpu().numpy().squeeze()\n",
    "                val_inputs = val_inputs.detach().cpu().numpy().squeeze()\n",
    "                val_labels = np.argmax(val_labels, axis=0)\n",
    "                # Display the slices\n",
    "                n = 4  # number of slices\n",
    "                fig, ax = plt.subplots(n, 3, figsize=(15, 10))\n",
    "                for i in range(n):\n",
    "                    # Show image slice\n",
    "                    ax[i, 0].imshow(val_inputs[:, :, i* val_inputs.shape[2]//n].T, vmin=0.0, vmax=0.75, cmap='gray')\n",
    "                    ax[i, 0].set_title('Image')\n",
    "                    ax[i, 0].axis('off')\n",
    "                    # Show label slice\n",
    "                    ax[i, 1].imshow(val_labels[:, :, i* val_labels.shape[2]//n].T, cmap='viridis')\n",
    "                    ax[i, 1].set_title('Label')\n",
    "                    ax[i, 1].axis('off')\n",
    "                    # Show model output slice\n",
    "                    ax[i, 2].imshow(val_outputs[:, :, i* val_outputs.shape[2]//n].T, cmap='viridis')\n",
    "                    ax[i, 2].set_title('Model output')\n",
    "                    ax[i, 2].axis('off')\n",
    "                # save the figure\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"val_output_{epoch+1}.png\", dpi=300, bbox_inches='tight')\n",
    "                \n",
    "print(f\"Training completed. Best validation mean dice: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "\n",
    "# Plot the loss and metric\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color='red')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color='green')\n",
    "plt.savefig(\"loss_metric.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our best model and evaluate performance in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and test\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_metric_model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    test_dice = 0\n",
    "    for test_data in test_loader:\n",
    "        test_inputs, test_labels = test_data[\"image\"].to(device), test_data[\"label\"].to(device)\n",
    "        test_outputs = sliding_window_inference(test_inputs, (64, 64, 64), 4, model, overlap=0.25)\n",
    "        test_outputs = torch.softmax(test_outputs, 1)  # Apply softmax\n",
    "        test_outputs = torch.argmax(test_outputs, dim=1, keepdim=True)  # Get class predictions\n",
    "        dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    dice_metric.reset()\n",
    "    print(f\"Test mean dice: {metric:.4f}\")\n",
    "\n",
    "    # Display the first test image, label, and model output\n",
    "    test_data = first(test_loader)\n",
    "    test_inputs, test_labels = test_data[\"image\"].to(device), test_data[\"label\"].to(device)\n",
    "    test_outputs = sliding_window_inference(test_inputs, (64, 64, 64), 4, model, overlap=0.25)\n",
    "    test_outputs = torch.softmax(test_outputs, 1)  # Apply softmax\n",
    "    test_outputs = torch.argmax(test_outputs, dim=1, keepdim=True)  # Get class predictions\n",
    "    # Convert output and label to numpy arrays and undo one-hot encoding\n",
    "    test_outputs = test_outputs.detach().cpu().numpy().squeeze()\n",
    "    test_labels = test_labels.detach().cpu().numpy().squeeze()\n",
    "    test_inputs = test_inputs.detach().cpu().numpy().squeeze()\n",
    "    test_labels = np.argmax(test_labels, axis=0)\n",
    "    # Display the slices\n",
    "    n = 4  # number of slices\n",
    "    fig, ax = plt.subplots(n, 3, figsize=(15, 10))\n",
    "    for i in range(n):\n",
    "        # Show image slice\n",
    "        ax[i, 0].imshow(test_inputs[:, :, i* test_inputs.shape[2]//n].T, vmin=0, vmax=0.75, cmap='gray')\n",
    "        ax[i, 0].set_title('Image')\n",
    "        ax[i, 0].axis('off')\n",
    "        # Show label slice\n",
    "        ax[i, 1].imshow(test_labels[:, :, i* test_labels.shape[2]//n].T, cmap='viridis')\n",
    "        ax[i, 1].set_title('Label')\n",
    "        ax[i, 1].axis('off')\n",
    "        # Show model output slice\n",
    "        ax[i, 2].imshow(test_outputs[:, :, i* test_outputs.shape[2]//n].T, cmap='viridis')\n",
    "        ax[i, 2].set_title('Model output')\n",
    "        ax[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"test_output.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsci_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
